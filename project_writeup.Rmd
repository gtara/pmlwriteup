Project Write-up for the course 
"Practical Machine Learning" (Jul 07 2014) project
========================================================

Background
========================================================
Short desc
Article ref
Data download link

Cleaning the data
========================================================

At first the data is loaded, empty or NA strings are replaced with R NA.
    
```{r Loading the data,message=FALSE, warning=FALSE,echo=TRUE, results='hide'}
badEntries<-c("","NA", "#DIV/0!")
traindata <- read.csv("pml-training.csv", na.strings=badEntries)
testdata <- read.csv("pml-testing.csv", na.strings=badEntries)
```

```{r Dimensions of the data, echo=TRUE,results='hold'}
dim(traindata)
```
The data that can be used for training is 19622 observations with 159 features and one reponse variable.

We analyse how prevalent are the NA values in the training data.
``` {r Identify empty columns, results='hide'}
countVectNA<- function(x) {sum(is.na(x))}
columnNA <- apply(traindata,2,countVectNA)
keepColumns<-(columnNA==0)
```


Since there are 160 features an efficient way to get an overview would be a simple plot.

```{r Empty columns plot, fig.width=7, fig.height=5}
plot(columnNA, xlab="Feature index", ylab="Count of NA values" )
```

We can see that the data is either provided or entirelly missing, so we do not have to deal with the missing data using methods as e.g. knnImpute, we just remove the empty columns. Also since this is not a time series analysis project, we are removing the time - related data.

```{r Reduce columns, results='hide' }
traindata <- traindata[,keepColumns]
testdata <- testdata[,keepColumns]
not_interesting<-c(1,2,3,4,5,6,7)
traindata<-traindata[,-not_interesting]
testdata<-testdata[,-not_interesting]
```

```{r, results='hide', echo=FALSE}
rm(not_interesting)
```

Feature reduction
=========================================================
```{r, results='hold'}
columncount<-dim(traindata)[2]
columncount
```

We are dealing with 52 features, which is computationally feasible. Nevertheless it would be interesting to see if features can be reduced and satisfactory accuracy can be achieved. We do so by looking at features that have some corelation to the class.


```{r}
delme<-data.frame(traindata[,-columncount], as.numeric(traindata[,columncount]))
corclass<-cor(delme)[,columncount]
corelated<-c(which(corclass > 0.05))
traindata<-traindata[,corelated]
traindata<-traindata[,corelated]
```

```{r, results='hide', echo=FALSE}
rm(corelated, delme)
```

This leaves us with the following 12 features
```{r, results='hold'}
names(traindata)
```

Training and testing
===========================================================
Since we cannot tune the model on testdata (only two submissions are allowed), we will be taking a part of the training dataset for testing. Using a part of the training data as testdata also reduces the out of sample error, which can be important in some algorithms(linear/logistic regression), less so in others(random forests). 

```{r, message=FALSE, warning=FALSE}
library(caret)
set.seed(314568)
train_index <- createDataPartition(y = traindata$classe, p=0.7,list=FALSE) 
mtrain<-traindata[train_index,]
mtest<-traindata[-train_index,]
```

We first look at the graphical information of a random subset of the data. 

```{r, fig.width=15, fig.height=15}
viz_index <- createDataPartition(y = traindata$classe, p=0.1,list=FALSE) 
pairs(traindata[viz_index,c(1:9, 13)])
```

We can identify that data is clustered, but some parameters are corelated. Random forest with principal component analysis would be a suitable machine learning algorithm for such data.

#modRfCenterSc<-train(classe~.,method = "rf", data = mtrain,preprocess=c("center", "scale"))
#modRfPca<-train(classe~.,method = "rf", data = mtrain,preprocess="pca")
#forest1<-randomForest(classe ~., data=mtrain, method="pca")
#preds<-predict(modFit, newdata=preptest)
#confusionMatrix(mtest$classe, preds)

